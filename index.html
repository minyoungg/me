<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>minyoung huh</title>

  <meta name="author" content="Minyoung Huh">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/mit_icon.ico">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>minyoung huh (jacob)</name>
              </p>
              <p>
                  I am a PhD student working on machine learning and computer vision at
                  <a href="https://www.csail.mit.edu/">MIT CSAIL</a> / <a href="https://ei.csail.mit.edu/">Embodied Intelligence</a> working with
                  <a href='http://web.mit.edu/phillipi/'>Phillip Isola</a> and
                  <a href='https://people.eecs.berkeley.edu/~pulkitag/'>Pulkit Agrawal</a>. I received my Bachelors from <a href='http://www.berkeley.edu/'>UC Berkeley</a>
                  advised under <a href="http://www.eecs.berkeley.edu/~efros/">Alexei (Alyosha) Efros</a> at
                  <a href='https://bair.berkeley.edu/'>Berkeley AI Research (BAIR)</a>.
                  Prior to joining BAIR, I worked with <a href='https://www.ece.cmu.edu/directory/department/faculty/C/Maysamreza_Chamanzar_9344.html'>Maysam Chamanzar</a>
                  and <a href='https://maharbizgroup.wordpress.com/'>Michel Maharbiz</a> at <a href="https://swarmlab.eecs.berkeley.edu/">Swarm Lab</a>.
              </p>

              <p>
              My current research interest is in the empirical theory of deep learning,
              the emergence of behavior & representation, and generative models.
              </p>


              <p style="text-align:center">
                <a href="mailto:minhuh@mit.edu">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=2k18_1IAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/minyoungg">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/leo-circle.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/me-circle.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0 35px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="./images/simplicity-bias-thumb.png" alt="blind-date" width="160" height="90">
            </td>
            <td width="75%" valign="middle">
              <a href="https://minyoungg.github.io/overparam/">
                <papertitle>The Low-Rank Simplicity Bias in Deep Networks</papertitle>
              </a>
              <br>
              <strong> Minyoung Huh</strong>,
              <a href="https://people.csail.mit.edu/hmobahi/"> Hossein Mobahi</a>,
              <a href="https://richzhang.github.io/"> Richard Zhang</a>,
							<a href="https://scholar.google.com/citations?user=7N-ethYAAAAJ&hl=en"> Brian Cheung</a>,
							<a href="https://people.csail.mit.edu/pulkitag/"> Pulkit Agrawal</a>,
							<a href="http://web.mit.edu/phillipi/"> Phillip Isola</a>
              <br>
              <em>arXiv</em>, 2021 (under review)
              <p>We conjecture that deep networks are implicitly biased to find lower rank solutions
                 and that these are the solutions that generalize well. We further demonstrate linear over-parameterization can
                 be used as an implicit regularizer to improve generalization without changing the effective model capacity.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="http://people.csail.mit.edu/minhuh/thumb/pix2latent_thumb.png" alt="blind-date" width="160" height="90">
            </td>
            <td width="75%" valign="middle">
              <a href="https://minyoungg.github.io/pix2latent/">
                <papertitle>Transforming and Projecting Images into Class-conditional Generative Networks</papertitle>
              </a>
              <br>
              <strong> Minyoung Huh</strong>,
              <a href="https://richzhang.github.io/"> Richard Zhang</a>,
							<a href="https://people.csail.mit.edu/junyanz/"> Jun-Yan Zhu</a>,
							<a href="http://people.csail.mit.edu/sparis/"> Sylvain Paris</a>,
							<a href="https://www.dgp.toronto.edu/~hertzman/"> Aaron Hertzmann</a>
              <br>
              <em>ECCV</em>, 2020 <font color="red"><strong>(Oral Presentation)</strong></font>
              <p>We propose a method for projecting images into the space of generative neural networks.
                 We optimize for transformation to counteract the model biases in a generative neural networks.
                 We further propose the use of hybrid (gradient + CMA-ES) optimization to improve model inversion.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="http://people.csail.mit.edu/minhuh/thumb/feedback_thumb.png" width="160" height="120">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Huh_Feedback_Adversarial_Learning_Spatial_Feedback_for_Improving_Generative_Adversarial_Networks_CVPR_2019_paper.pdf">
                <papertitle>Feedback Adversarial Learning: Spatial Feedback for Improving Generative Adversarial Networks</papertitle>
              </a>
              <br>
              <strong> Minyoung Huh</strong>,
              <a href="https://shaohua0116.github.io/">Shao-Hua Sun </a>,
							<a href="http://people.eecs.berkeley.edu/~nzhang/">Ning Zhang </a>
              <br>
              <em>CVPR</em>, 2019
              <br>
              <p> We propose feedback adversarial learning (FAL) framework that
                  can improve existing generative adversarial networks by leveraging
                  spatial feedback from the discriminator into generative process.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="http://people.csail.mit.edu/minhuh/thumb/forensics_thumb.png" width="160" height="100">
            </td>
            <td width="75%" valign="middle">
              <a href="https://minyoungg.github.io/selfconsistency/">
                <papertitle>Fighting Fake News: Detecting Malicious Image Manipulations via Learned Self-Consistency</papertitle>
              </a>
              <br>
              <strong>Minyoung Huh</strong>,
              <a href="https://andrewhliu.github.io/"> Andrew Liu*</a>,
              <a href="http://www.eecs.berkeley.edu/~efros/"> Alexei A. Efros</a>,
              <a href="http://andrewowens.com/"> Andrew Owens</a>
              <br>
              <em>ECCV</em>, 2018
              <br>
              <p>We propose a learning algorithm for detecting visual image manipulations that is trained only using a large dataset of real photographs.
                 The algorithm uses the automatically recorded photo EXIF metadata as supervisory signal for training a model to determine whether an image is self-consistent</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="http://people.csail.mit.edu/minhuh/thumb/nvs_thumb.png" alt="blind-date" width="160" height="90">
            </td>
            <td width="75%" valign="middle">
              <a href="https://shaohua0116.github.io/Multiview2Novelview/">
                <papertitle>Multi-view to Novel view: Synthesizing Views via Self-Learned Confidence</papertitle>
              </a>
              <br>
              <a href="https://shaohua0116.github.io/"> Shao-Hua Sun </a>,
							<strong>Minyoung Huh</strong>,
							<a href="https://andrewliao11.github.io/"> Yuan-Hong Liao </a>,
							<a href="http://people.eecs.berkeley.edu/~nzhang/"> Ning Zhang </a>,
							<a href="http://www-bcf.usc.edu/~limjj/"> Joseph J. Lim</a>
              <br>
              <em>ECCV</em>, 2018
              <br>
              <p> We propose an end-to-end trainable framework that learns to exploit multiple viewpoints
                  to synthesize a novel view without any 3D supervision.
                  Our model consists of a flow prediction module and a pixel generation module to directly leverage
                  information presented in source views as well as hallucinate missing pixels from statistical priors.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="http://people.csail.mit.edu/minhuh/thumb/imagenet_thumb.PNG" width="160" height="110">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/1608.08614v2.pdf">
                <papertitle>What makes ImageNet good for Transfer Learning?</papertitle>
              </a>
              <br>
              <strong>Minyoung Huh</strong>,
              <a href="https://people.eecs.berkeley.edu/~pulkitag/"> Pulkit Agrawal</a>,
							<a href="http://www.eecs.berkeley.edu/~efros/"> Alexei A. Efros</a>
              <br>
              <em>NeurIPS workshop</em>, 2018
              <br>
              <p>This work provides an empirical investigation into the various facets of this question,
                such as looking at the importance of the amount of examples, number of classes, balance between images-per-class and classes,
                and the role of fine and coarse grained recognition. We pre-train CNN features on various subsets of the ImageNet dataset and
                evaluate transfer performance on a variety of standard vision tasks.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="http://people.csail.mit.edu/minhuh/thumb/ultrasound_nature_thumb.gif" width="160" height="120">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.nature.com/articles/s41467-018-07856-w?fbclid=IwAR2NUi31PEf44XXUVajIG7j8qntOaYSQ_6dwT-wczLvn4wuFto5eNFbImk4">
                <papertitle>Ultrasonic sculpting of virtual, steerable optical waveguides in tissue</papertitle>
              </a>
              <br>
              <a href='https://www.ece.cmu.edu/directory/department/faculty/C/Maysamreza_Chamanzar_9344.html'>Maysam Chamanzar</a>,
               Matteo Giuseppe Scopelliti,
               Julien Bloch,
               Ninh Do,
              <strong> Minyoung Huh</strong>,
              <a href="https://scholar.google.com/citations?user=dc40_-AAAAAJ&hl=en"> Dongjin Seo</a>,
               Jillian Iafrati,
               Vikaas S. Sohal,
              <a href="https://me.berkeley.edu/people/m-reza-alam/"> Mohammad-Reza Alam</a>,
              <a href="https://maharbizgroup.wordpress.com/"> Michel M. Maharbiz</a>
              <br>
              <em>Nature Communications</em>, 2019
              <br>
              <p> We demonstrate that ultrasound can be used to define and steer the trajectory of light within scattering
                  media by exploiting local pressure differences created by acoustic waves that result in refractive index contrasts.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="http://people.csail.mit.edu/minhuh/thumb/cleo_thumb.PNG" width="160" height="100">
            </td>
            <td width="75%" valign="middle">
              <a href='https://www.osapublishing.org/abstract.cfm?uri=cleo_at-2015-JW2A.90'>
                <papertitle>Virtual Acousto-optic Beam Paths for Steerable Deep-tissue Optical Stimulation and Imaging</papertitle>
              </a>
              <br>
              <a href='https://www.ece.cmu.edu/directory/department/faculty/C/Maysamreza_Chamanzar_9344.html'> Maysam Chamanzar</a>,
              <strong> Minyoung Huh</strong>,
               Ninh Do,
              <a href="https://me.berkeley.edu/people/m-reza-alam/"> Mohammad-Reza Alam</a>,
              <a href="https://maharbizgroup.wordpress.com/"> Michel M. Maharbiz</a>
              <br>
              <em>CLEO</em>, 2016 & <em>SfN</em>, 2016
              <br>
              <p> We present the first non-invasive methodology for optical delivery and steering deep inside the brain through
                  creating reconfigurable light paths by ultrasonic waves via modulating the refractive and diffractive properties of the medium.</p>
            </td>
          </tr>

        </table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Miscellaneous</heading>
              <p>Fun little <a href="https://www.newyorker.com/magazine/2018/11/12/in-the-age-of-ai-is-seeing-still-believing">The New Yorker article</a> on image generation that I participated in (very briefly). </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0 35px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


      </td>
    </tr>
  </table>
</body>

</html>
